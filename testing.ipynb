{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from model.layers import Embedding, Sigmoid, Softmax, Linear, BCELossWithSigmoid\n",
    "from model.HS_model import HS_skipgram\n",
    "from preprocess import *\n",
    "from optim.optimizer import SGD\n",
    "from itertools import repeat\n",
    "import time\n",
    "import pickle\n",
    "from eval import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from collections import Counter\n",
    "from fuel.utils import find_in_data_path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Huffman_Tree: 100%|████████████████████████████████████████████████████████| 692000/692000 [00:02<00:00, 282448.70it/s]\n",
      "Padding Paths: 100%|███████████████████████████████████████████████████████| 692001/692001 [00:01<00:00, 565943.68it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 300\n",
    "sample_size = 5\n",
    "path = \"./1-billion-word/training-monolingual.tokenized.shuffled\"\n",
    "\n",
    "with open(\"./data.pickle\", 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "\n",
    "word2idx = save[\"word2idx\"]\n",
    "idx2word = save[\"idx2word\"]\n",
    "count = save[\"count\"]\n",
    "file_path = save[\"file_path\"]\n",
    "\n",
    "node, max_depth = Huffman_Tree(count)\n",
    "\n",
    "\n",
    "def batch_words(paths):\n",
    "    words = []\n",
    "    for path in paths:\n",
    "        word = recall_word(path)\n",
    "        words += word\n",
    "\n",
    "    return words\n",
    "\n",
    "#sample_size random하게 바꿈. --> 한 corpus에서 더 다양하게 보는게 가능!\n",
    "model = HS_skipgram(len(word2idx), 300, lr = 0.0025)\n",
    "criterion = BCELossWithSigmoid()\n",
    "optimizer = SGD(lr = 0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at\n",
      "in [0.399965]\n",
      "on [0.40077132]\n",
      "At [0.4053536]\n",
      "at [1.]\n",
      "it\n",
      "if [0.62119794]\n",
      "what [0.6283134]\n",
      "not [0.63258946]\n",
      "it [1.0000001]\n",
      "by\n",
      "with [0.3478323]\n",
      "under [0.36758286]\n",
      "has [0.40016577]\n",
      "by [1.0000001]\n",
      "from\n",
      "with [0.40144348]\n",
      "in [0.41637623]\n",
      "into [0.42502478]\n",
      "from [1.0000002]\n",
      "be\n",
      "it [0.50501835]\n",
      "not [0.540478]\n",
      "take [0.55863816]\n",
      "be [1.0000001]\n",
      "have\n",
      "were [0.5017862]\n",
      "had [0.52107686]\n",
      "they [0.52164423]\n",
      "have [1.]\n",
      "he\n",
      "Mr [0.5843762]\n",
      "He [0.5843973]\n",
      "she [0.72300315]\n",
      "he [1.]\n",
      "has\n",
      "also [0.44568735]\n",
      "been [0.47705603]\n",
      "had [0.5020534]\n",
      "has [1.]\n",
      "his\n",
      "after [0.49108356]\n",
      "him [0.5421071]\n",
      "her [0.64442587]\n",
      "his [1.]\n",
      "are\n",
      "these [0.5485845]\n",
      "many [0.5752139]\n",
      "some [0.57674885]\n",
      "are [1.0000001]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(path, model, word2idx, idx2word):\n",
    "    with open(\"./bestmodel.pickle\", 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "\n",
    "\n",
    "    model.params = x\n",
    "    for i in range(20,30):\n",
    "        #ex = np.random.choice(len(word2idx), 1)[0]\n",
    "        model.query(idx2word[i], word2idx, idx2word, top = 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluate(\"\",model,word2idx, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./bestmodel.pickle\", 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "\n",
    "\n",
    "model.params = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem, syn = test_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a604ebab9ac5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcal_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msyn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\Word2vec\\Word2Vec_Study\\eval.py\u001b[0m in \u001b[0;36mcal_score\u001b[1;34m(sample, semantic_words, syntatic_words, model, word2idx, idx2word)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mans\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mans\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "cal_score(100, sem,syn, model, word2idx, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Counter([1,2,31,1,23,12,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10675"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem.shape\n",
    "syn.shape\n",
    "len(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 72, 71, ..., 26, 36, 99],\n",
       "       [ 0, 72, 71, ..., 26, 36, 99],\n",
       "       [ 0, 72, 71, ..., 26, 36, 99],\n",
       "       ...,\n",
       "       [ 0, 72, 71, ..., 26, 36, 99],\n",
       "       [ 0, 72, 71, ..., 26, 36, 99],\n",
       "       [ 0, 72, 71, ..., 26, 36, 99]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t / np.linalg.norm(t, axis = 1, keepdims = True)).argsort(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2, 2: 1, 31: 1, 23: 1, 12: 1, 22: 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(1, 2): 1, (2, 1): 1, (31, 1): 1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./1-billion-word/training-monolingual.tokenized.shuffled/news.en-00001-of-00100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "with open(path, 'r', encoding = \"UTF8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        words.append(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'U.S.',\n",
       " 'Centers',\n",
       " 'for',\n",
       " 'Disease',\n",
       " 'Control',\n",
       " 'and',\n",
       " 'Prevention',\n",
       " 'initially',\n",
       " 'advised',\n",
       " 'school',\n",
       " 'systems',\n",
       " 'to',\n",
       " 'close',\n",
       " 'if',\n",
       " 'outbreaks',\n",
       " 'occurred',\n",
       " ',',\n",
       " 'then',\n",
       " 'reversed',\n",
       " 'itself',\n",
       " ',',\n",
       " 'saying',\n",
       " 'the',\n",
       " 'apparent',\n",
       " 'mildness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'virus',\n",
       " 'meant',\n",
       " 'most',\n",
       " 'schools',\n",
       " 'and',\n",
       " 'day',\n",
       " 'care',\n",
       " 'centers',\n",
       " 'should',\n",
       " 'stay',\n",
       " 'open',\n",
       " ',',\n",
       " 'even',\n",
       " 'if',\n",
       " 'they',\n",
       " 'had',\n",
       " 'confirmed',\n",
       " 'cases',\n",
       " 'of',\n",
       " 'swine',\n",
       " 'flu',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
